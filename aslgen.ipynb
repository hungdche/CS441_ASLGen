{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcfa01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from natsort import natsorted\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17ff31e",
   "metadata": {},
   "source": [
    "### Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf55f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset/asl_dataset/train\"\n",
    "CLASSES = natsorted(os.listdir(DATASET_PATH))\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "print(f\"Classes: {CLASSES}\")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std= [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "            transforms.Resize(256),  # resize to 224x224 because that's the size of ImageNet images\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std= [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "class ASLDataset(Dataset):\n",
    "    def __init__(self, dataset_path, mode=\"train\", single_class=None, transform=None, generated_path=False):\n",
    "        self.dataset_path = dataset_path\n",
    "        if transform:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = train_transform if mode==\"train\" else test_transform\n",
    "            \n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        self.mode = mode\n",
    "        self.single_class = single_class\n",
    "        self.generated_path = generated_path\n",
    "\n",
    "        if not single_class:\n",
    "            images = glob(os.path.join(dataset_path, \"*\", \"*.png\"))\n",
    "            images = natsorted(images)\n",
    "        else:\n",
    "            images = glob(os.path.join(dataset_path, single_class, \"*.png\"))\n",
    "            images = natsorted(images)\n",
    "            \n",
    "        # if mode is none, use all data\n",
    "        if self.mode != \"none\":\n",
    "            # last 5 frames for test, rest for train\n",
    "            if self.mode == \"train\":\n",
    "                images = images[: -5]\n",
    "            if self.mode == \"test\":\n",
    "                images = images[-5 :]\n",
    "\n",
    "        for img_path in images:\n",
    "            \n",
    "            label = img_path.split('/')[-2]            \n",
    "            self.image_paths.append(img_path)\n",
    "            self.labels.append(CLASSES.index(label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "train_dataset = ASLDataset(DATASET_PATH, mode=\"none\")\n",
    "\n",
    "# visualize a few samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    rand_sample = random.randint(0, len(train_dataset)-1)\n",
    "    img, label = train_dataset[rand_sample]\n",
    "    axes[i//5, i%5].imshow(img.permute(1, 2, 0).numpy() * 0.229 + 0.485)  # unnormalize for display\n",
    "    axes[i//5, i%5].set_title(CLASSES[label])\n",
    "    axes[i//5, i%5].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7317100d",
   "metadata": {},
   "source": [
    "### Small VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb2847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import TinyVAE, train_vae, test_vae, vae_loss\n",
    "\n",
    "def train_vae_model(cfg, training_cfg, ALPHABET_CLASS): \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TinyVAE(cfg)\n",
    "    model = model.to(device)\n",
    "\n",
    "    save_dir = 'checkpoints/tiny_vae/'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    vae_transform = transforms.Compose([\n",
    "        transforms.Resize(cfg['input_res']), \n",
    "        transforms.RandomAffine(degrees=5, translate=(0.05, 0.05)), \n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = ASLDataset(DATASET_PATH, mode=\"none\", single_class=ALPHABET_CLASS, transform=vae_transform)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=training_cfg['batch_size'], shuffle=True, drop_last=True)\n",
    "    fid_gt_folder = f'asl_dataset/{ALPHABET_CLASS}'\n",
    "\n",
    "    num_epochs = training_cfg['num_epochs']\n",
    "    test_interval = training_cfg['test_interval']\n",
    "\n",
    "    # TO DO: set initial learning rate\n",
    "    learn_rate = training_cfg['lr']\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learn_rate, eps=1e-15)\n",
    "\n",
    "    # TO DO: define your learning rate scheduler, e.g. StepLR\n",
    "    lr_scheduler =  torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "    criterion = vae_loss\n",
    "\n",
    "    epochs_list = []\n",
    "    train_recon_losses = []\n",
    "    train_kl_losses = []\n",
    "    test_fid_scores = []\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for epoch in tqdm(range(num_epochs), total=num_epochs, desc=\"Training ...\", position=1):\n",
    "\n",
    "        # Compute KL beta\n",
    "        kl_max = training_cfg['kl_reg']\n",
    "        def kl_weight(epoch, warmup=100):\n",
    "            return min(kl_max, epoch / warmup * kl_max)\n",
    "\n",
    "        # Train the network for one epoch\n",
    "        loss_dict = train_vae(train_dataloader, model, criterion, optimizer, beta=kl_weight(epoch), alphabet_class=ALPHABET_CLASS, epoch=epoch)\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        # TO DO (see step() in lr_scheduler)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # print(f'Loss for Training on epoch {str(epoch)} is {str(train_loss)}')\n",
    "\n",
    "        # Get the train accuracy and test loss/accuracy\n",
    "        if(epoch%test_interval==0 or epoch==1 or epoch==num_epochs-1):\n",
    "            # print('Evaluating Network')\n",
    "\n",
    "            epochs_list.append(epoch)\n",
    "\n",
    "            # Get training accuracy and loss\n",
    "            output_folder = f'training_res/vae/{ALPHABET_CLASS}/samples/epoch_{epoch}'\n",
    "            fid_score = test_vae(model, output_folder, num_samples=50, compute_fid=training_cfg['compute_fid'], img_folder=fid_gt_folder)\n",
    "            train_recon_losses.append(loss_dict['recon_loss'])\n",
    "            train_kl_losses.append(loss_dict['kl_loss'])\n",
    "            if fid_score is not None:\n",
    "                test_fid_scores.append(fid_score)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5112568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training unconditional VAE\n",
    "\n",
    "ALPHABET_CLASS = '5'\n",
    "\n",
    "cfg = {\n",
    "    'img_channels': 3,\n",
    "    'latent_dim': 32, \n",
    "    'enc_sizes': [16, 32, 64, 128, 256],\n",
    "    'dec_sizes': [256, 128, 64, 32],\n",
    "    'input_res': 128,\n",
    "    'is_conditional': False,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "}\n",
    "\n",
    "training_cfg = {\n",
    "    'batch_size': 16, \n",
    "    'num_epochs': 5_000,\n",
    "    'test_interval': 200,\n",
    "    'lr': 3e-4,\n",
    "    'kl_reg': 0.05,\n",
    "    'compute_fid': False,\n",
    "}\n",
    "\n",
    "train_vae_model(cfg, training_cfg, ALPHABET_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca433c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 1\n",
    "num_cols = 10\n",
    "num_phases = 5\n",
    "seed = 0\n",
    "transition_frames = 25\n",
    "static_frames = 5\n",
    "resolution = cfg['input_res']\n",
    "output = f'transition_{ALPHABET_CLASS}_vae.gif'\n",
    "\n",
    "np.random.seed(seed)\n",
    "output_seq = []\n",
    "batch_size = num_rows * num_cols\n",
    "latent_size = cfg['latent_dim']\n",
    "latents = [np.random.randn(batch_size, latent_size) for _ in range(num_phases)]\n",
    "\n",
    "model = TinyVAE(cfg)\n",
    "model.load_state_dict(torch.load(f'training_res/vae/{ALPHABET_CLASS}/samples/epoch_{training_cfg[\"num_epochs\"]-1}/model.pth'))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def to_image_grid(outputs):\n",
    "    outputs = np.reshape(outputs, [num_rows, num_cols, *outputs.shape[1:]])\n",
    "    outputs = np.concatenate(outputs, axis=1)\n",
    "    outputs = np.concatenate(outputs, axis=1)\n",
    "    return Image.fromarray(outputs).resize((resolution * num_cols, resolution * num_rows), Image.LANCZOS)\n",
    "\n",
    "def generate(dlatents):\n",
    "    images = model.decode(dlatents.to(device))\n",
    "    images = (images.permute(0, 2, 3, 1) * 255).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
    "    return to_image_grid(images)\n",
    "\n",
    "for i in range(num_phases):\n",
    "    dlatents0 = torch.from_numpy(latents[i - 1]).to(device).float()\n",
    "    dlatents1 = torch.from_numpy(latents[i]).to(device).float()\n",
    "\n",
    "    for j in range(transition_frames):\n",
    "        dlatents = (dlatents0 * (transition_frames - j) + dlatents1 * j) / transition_frames\n",
    "        output_seq.append(generate(dlatents))\n",
    "    output_seq.extend([generate(dlatents1)] * static_frames)\n",
    "\n",
    "if not output.endswith('.gif'):\n",
    "    output += '.gif'\n",
    "output_seq[0].save(output, save_all=True, append_images=output_seq[1:], optimize=False, duration=50, loop=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c211c35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and dump 500 samples from the trained GAN model\n",
    "output_folder = f'generated_images/vae/{ALPHABET_CLASS}/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "num_samples = 500\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(num_samples, cfg['latent_dim'], device=device)\n",
    "    samples = model.decode(z).cpu()\n",
    "    samples = (samples.permute(0, 2, 3, 1) * 255).clamp(0, 255).to(torch.uint8).numpy()\n",
    "for i in range(samples.shape[0]):\n",
    "    img = transforms.ToPILImage()(samples[i])\n",
    "    img.save(os.path.join(output_folder, f\"sample_{i}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a77b8ba",
   "metadata": {},
   "source": [
    "### GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05db61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Generator, Discriminator, train_gan, test_gan\n",
    "\n",
    "def train_gan_model(cfg, training_cfg, ALPHABET_CLASS):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Models\n",
    "    G = Generator(cfg).to(device)\n",
    "    D = Discriminator(cfg).to(device)\n",
    "\n",
    "    save_dir = 'checkpoints/gan/'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Dataset\n",
    "    gan_transform = transforms.Compose([\n",
    "        transforms.Resize(cfg['input_res']),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = ASLDataset(\n",
    "        DATASET_PATH,\n",
    "        mode=\"none\",\n",
    "        single_class=ALPHABET_CLASS,\n",
    "        transform=gan_transform\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=training_cfg['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "    fid_gt_folder = f'asl_dataset/{ALPHABET_CLASS}'\n",
    "\n",
    "    # Optimizers\n",
    "    g_optimizer = torch.optim.Adam(\n",
    "        G.parameters(),\n",
    "        lr=training_cfg['lr'],\n",
    "        betas=(0.0, 0.99)\n",
    "    )\n",
    "\n",
    "    d_optimizer = torch.optim.Adam(\n",
    "        D.parameters(),\n",
    "        lr=training_cfg['lr'],\n",
    "        betas=(0.0, 0.99)\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = training_cfg['num_epochs']\n",
    "    test_interval = training_cfg['test_interval']\n",
    "    latent_dim = cfg['latent_dim']\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), total=num_epochs, desc=\"Training GAN ...\", position=1):\n",
    "\n",
    "        loss_dict = train_gan(\n",
    "            train_loader=train_dataloader,\n",
    "            G=G,\n",
    "            D=D,\n",
    "            g_optimizer=g_optimizer,\n",
    "            d_optimizer=d_optimizer,\n",
    "            epoch=epoch,\n",
    "            latent_dim=latent_dim,\n",
    "            use_diffaugment=True\n",
    "        )\n",
    "\n",
    "        if epoch % test_interval == 0 or epoch == 1 or epoch == num_epochs - 1:\n",
    "\n",
    "            output_folder = f'training_res/gan/{ALPHABET_CLASS}/samples/epoch_{epoch}'\n",
    "            fid_score = test_gan(\n",
    "                G,\n",
    "                save_img_folder=output_folder,\n",
    "                latent_dim=latent_dim,\n",
    "                num_samples=50,\n",
    "                compute_fid=training_cfg['compute_fid'],\n",
    "                img_folder=fid_gt_folder\n",
    "            )\n",
    "\n",
    "            # Save checkpoints\n",
    "            torch.save(G.state_dict(), os.path.join(save_dir, f'G_epoch_{epoch}.pth'))\n",
    "            torch.save(D.state_dict(), os.path.join(save_dir, f'D_epoch_{epoch}.pth'))\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e62c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET_CLASS = '5'\n",
    "\n",
    "cfg = {\n",
    "    'img_channels': 3,\n",
    "    'latent_dim': 16, \n",
    "    'gen_sizes': [512, 256, 128, 64],\n",
    "    'disc_sizes': [64, 128, 256, 512],\n",
    "    'input_res': 128,\n",
    "    'is_conditional': False,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "}\n",
    "\n",
    "training_cfg = {\n",
    "    'batch_size': 8, # 8 for single class, 128 for conditional VAE\n",
    "    'num_epochs': 4000, # 4000 for unconditional VAE, 400 for conditional VAE\n",
    "    'test_interval': 400, # 40 for conditional VAE, 400 for unconditional VAE\n",
    "    'lr': 1e-4,\n",
    "    'compute_fid': False,\n",
    "}\n",
    "\n",
    "G = train_gan_model(cfg, training_cfg, ALPHABET_CLASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c91b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 1\n",
    "num_cols = 10\n",
    "num_phases = 5\n",
    "seed = 0\n",
    "transition_frames = 25\n",
    "static_frames = 5\n",
    "resolution = cfg['input_res']\n",
    "output = f'transition_{ALPHABET_CLASS}.gif'\n",
    "\n",
    "np.random.seed(seed)\n",
    "output_seq = []\n",
    "batch_size = num_rows * num_cols\n",
    "latent_size = cfg['latent_dim']\n",
    "latents = [np.random.randn(batch_size, latent_size) for _ in range(num_phases)]\n",
    "\n",
    "G = Generator(cfg).to(device)\n",
    "G.load_state_dict(torch.load(f'training_res/gan/{ALPHABET_CLASS}/samples/epoch_{training_cfg[\"num_epochs\"]-1}/G.pth'))\n",
    "G.eval()\n",
    "\n",
    "def to_image_grid(outputs):\n",
    "    outputs = np.reshape(outputs, [num_rows, num_cols, *outputs.shape[1:]])\n",
    "    outputs = np.concatenate(outputs, axis=1)\n",
    "    outputs = np.concatenate(outputs, axis=1)\n",
    "    return Image.fromarray(outputs).resize((resolution * num_cols, resolution * num_rows), Image.LANCZOS)\n",
    "\n",
    "def generate(dlatents, c=None):\n",
    "    images = G(dlatents.to(device))\n",
    "    images = (images.permute(0, 2, 3, 1) * 255).clamp(0, 255).to(torch.uint8).cpu().numpy()\n",
    "    return to_image_grid(images)\n",
    "\n",
    "for i in range(num_phases):\n",
    "    dlatents0 = torch.from_numpy(latents[i - 1]).to(device).float()\n",
    "    dlatents1 = torch.from_numpy(latents[i]).to(device).float()\n",
    "\n",
    "    for j in range(transition_frames):\n",
    "        dlatents = (dlatents0 * (transition_frames - j) + dlatents1 * j) / transition_frames\n",
    "        output_seq.append(generate(dlatents))\n",
    "    output_seq.extend([generate(dlatents1)] * static_frames)\n",
    "\n",
    "if not output.endswith('.gif'):\n",
    "    output += '.gif'\n",
    "output_seq[0].save(output, save_all=True, append_images=output_seq[1:], optimize=False, duration=50, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6219dade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and dump 500 samples from the trained GAN model\n",
    "output_folder = f'generated_images/ucgan/{ALPHABET_CLASS}/'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "num_samples = 500\n",
    "G.eval()\n",
    "with torch.no_grad():\n",
    "    z = torch.randn(num_samples, cfg['latent_dim'], device=device)\n",
    "    samples = G(z).cpu()\n",
    "    samples = (samples.permute(0, 2, 3, 1) * 255).clamp(0, 255).to(torch.uint8).numpy()\n",
    "for i in range(samples.shape[0]):\n",
    "    img = transforms.ToPILImage()(samples[i])\n",
    "    img.save(os.path.join(output_folder, f\"sample_{i}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe3f17",
   "metadata": {},
   "source": [
    "### Downstream detection tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c9a22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"dataset/asl_dataset\"\n",
    "CLASSES = natsorted(os.listdir(f'{DATASET_PATH}/train'))\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "print(f\"Classes: {CLASSES}\")\n",
    "\n",
    "\n",
    "GAN_GEN_DATASET_PATH = \"generated_images/ucgan/\"\n",
    "VAE_GEN_DATASET_PATH = \"generated_images/vae/\"\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "class HybridASLDataset(Dataset):\n",
    "    def __init__(self, dataset_path, mode=\"train\", transform=None, gen_mode=\"none\", no_real_data=False):\n",
    "        \n",
    "        self.dataset_path = dataset_path\n",
    "\n",
    "        print(f\"Initializing HybridASLDataset with gen_mode={gen_mode}, no_real_data={no_real_data}, mode={mode}\")\n",
    "\n",
    "        assert gen_mode in [\"none\", \"ucgan\", \"vae\"], \"gen_mode must be one of 'none', 'ucgan', or 'vae'\"\n",
    "        assert mode in [\"train\", \"test\"], \"mode must be either 'train' or 'test'\"\n",
    "        assert not (no_real_data and mode!=\"train\"), \"no_real_data can only be True in train mode\"\n",
    "\n",
    "        self.gen_dataset_path = None\n",
    "        if gen_mode == \"ucgan\":\n",
    "            self.gen_dataset_path = GAN_GEN_DATASET_PATH\n",
    "        elif gen_mode == \"vae\":\n",
    "            self.gen_dataset_path = VAE_GEN_DATASET_PATH\n",
    "\n",
    "        if transform:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self.transform = train_transform if mode==\"train\" else test_transform\n",
    "            \n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.is_generated = []\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gen_mode = gen_mode\n",
    "        self.no_real_data = no_real_data\n",
    "\n",
    "        images = glob(os.path.join(dataset_path, mode, \"*\", \"*.png\"))\n",
    "        images = natsorted(images)\n",
    "\n",
    "        if not self.no_real_data:\n",
    "            for img_path in images:\n",
    "                label = img_path.split('/')[-2]            \n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(CLASSES.index(label))\n",
    "                self.is_generated.append(0)\n",
    "\n",
    "        # generated images for train only\n",
    "        if self.gen_dataset_path and self.mode == \"train\":\n",
    "            for class_name in CLASSES:\n",
    "                gen_images = glob(os.path.join(self.gen_dataset_path, class_name, \"*.png\"))\n",
    "                for img_path in gen_images:\n",
    "                    label = class_name\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.labels.append(CLASSES.index(label))\n",
    "                    self.is_generated.append(1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        is_generated = self.is_generated[idx]\n",
    "        \n",
    "        return image, label, is_generated\n",
    "\n",
    "train_dataset = HybridASLDataset(DATASET_PATH, mode=\"train\", gen_mode=\"none\")\n",
    "print(DATASET_PATH)\n",
    "\n",
    "# visualize a few samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    rand_sample = random.randint(0, len(train_dataset)-1)\n",
    "    img, label, _ = train_dataset[rand_sample]\n",
    "    axes[i//5, i%5].imshow(img.permute(1, 2, 0).numpy() * 0.229 + 0.485)  # unnormalize for display\n",
    "    axes[i//5, i%5].set_title(CLASSES[label])\n",
    "    axes[i//5, i%5].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39221535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import ResNetModel, SimpleCNN, train_clas, test_clas, save_checkpoint, load_model, plot_results\n",
    "\n",
    "\n",
    "def train_classification_model(model_type=\"tinycnn\", data_regime=\"mixed\", gen_model=\"ucgan\", training_cfg=None):\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    assert model_type in [\"tinycnn\", \"resnet\"], \"model_type must be either 'tinycnn' or 'resnet'\"\n",
    "    assert data_regime in [\"real_only\", \"gen_only\", \"mixed\"], \"data_regime must be one of 'real_only', 'gen_only', or 'mixed'\"\n",
    "    data_kwargs = {}\n",
    "    if data_regime == \"real_only\":\n",
    "        data_kwargs['no_real_data'] = False\n",
    "        data_kwargs['gen_mode'] = \"none\"\n",
    "    elif data_regime == \"gen_only\":\n",
    "        data_kwargs['no_real_data'] = True\n",
    "        data_kwargs['gen_mode'] = gen_model\n",
    "    elif data_regime == \"mixed\":\n",
    "        data_kwargs['no_real_data'] = False\n",
    "        data_kwargs['gen_mode'] = gen_model\n",
    "\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(128),\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "    ])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.Resize(128),  # resize to 128x128\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_dataset = HybridASLDataset(\n",
    "        DATASET_PATH,\n",
    "        mode=\"train\",\n",
    "        transform=train_transform,\n",
    "        **data_kwargs\n",
    "    )\n",
    "\n",
    "    eval_dataset = HybridASLDataset(\n",
    "        DATASET_PATH,\n",
    "        mode=\"test\",\n",
    "        transform=test_transform,\n",
    "        gen_mode=\"none\"\n",
    "    )\n",
    "\n",
    "    model_arch = ResNetModel if model_type == \"resnet\" else SimpleCNN\n",
    "    model = model_arch(NUM_CLASSES)\n",
    "    model = model.to(device)\n",
    "\n",
    "    save_dir = f'checkpoints/classification/{model_type}_{data_regime}_{gen_model}/'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=training_cfg['batch_size'], shuffle=True)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    num_epochs = training_cfg['num_epochs']\n",
    "    test_interval = training_cfg['test_interval']\n",
    "\n",
    "    # TO DO: set initial learning rate\n",
    "    learn_rate = training_cfg['lr']\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learn_rate, eps=1e-15)\n",
    "\n",
    "    # TO DO: define your learning rate scheduler, e.g. StepLR\n",
    "    # https://pytorch.org/docs/stable/optim.html#module-torch.optim.lr_scheduler\n",
    "    lr_scheduler =  torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(reduce='none')\n",
    "\n",
    "    epochs_list = []\n",
    "    train_losses = []\n",
    "    train_accuracy_list = []\n",
    "    test_losses = []\n",
    "    test_accuracy_list = []\n",
    "\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for epoch in tqdm(range(num_epochs), total=num_epochs, desc=\"Training ...\", position=1):\n",
    "\n",
    "        # Train the network for one epoch\n",
    "        train_loss = train_clas(train_dataloader, model, criterion, optimizer, epoch, fake_weight_max=0.1)\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # print(f'Loss for Training on epoch {str(epoch)} is {str(train_loss)}')\n",
    "\n",
    "        # Get the train accuracy and test loss/accuracy\n",
    "        if(epoch%test_interval==0 or epoch==1 or epoch==num_epochs-1):\n",
    "            print('Evaluating Network')\n",
    "\n",
    "            epochs_list.append(epoch)\n",
    "\n",
    "            # Get training accuracy and loss\n",
    "            train_accuracy, train_loss = test_clas(train_dataloader, model, criterion)\n",
    "            train_losses.append(train_loss)\n",
    "            train_accuracy_list.append(train_accuracy)\n",
    "\n",
    "            print(f'Training accuracy on epoch {str(epoch)} is {str(train_accuracy)}')\n",
    "\n",
    "            # Get test accuracy and loss (use test loader)\n",
    "            # TO DO\n",
    "            test_accuracy, test_loss = test_clas(eval_dataloader, model, criterion)\n",
    "            test_losses.append(test_loss)\n",
    "            test_accuracy_list.append(test_accuracy)\n",
    "\n",
    "            print(f'Test (val) accuracy on epoch {str(epoch)} is {str(test_accuracy)}')\n",
    "\n",
    "            # Checkpoints are used to save the model with best validation accuracy\n",
    "            if test_accuracy >= max(test_accuracy_list):\n",
    "              print(\"Saving Model\")\n",
    "              save_checkpoint(save_dir, model, save_name = 'best_model.pth') # Save model with best performance\n",
    "\n",
    "    return model, epochs_list, train_losses, train_accuracy_list, test_losses, test_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ba24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    'model_type': 'tinycnn',\n",
    "    'data_regime': 'real_only',\n",
    "    'gen_model': 'none',\n",
    "}\n",
    "\n",
    "training_cfg = {\n",
    "    'batch_size': 128, \n",
    "    'num_epochs': 100,\n",
    "    'test_interval': 20,\n",
    "    'lr': 5e-5,\n",
    "}\n",
    "\n",
    "model, epochs_list, train_losses, train_accuracy_list, test_losses, test_accuracy_list = \\\n",
    "  train_classification_model(training_cfg=training_cfg, **cfg)\n",
    "\n",
    "# plot\n",
    "train_accuracy_list_items = [x.item() for x in train_accuracy_list]\n",
    "test_accuracy_list_items = [x.item() for x in test_accuracy_list]\n",
    "plot_results(epochs_list, train_accuracy_list_items, test_accuracy_list_items, \"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d0e174",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "radarsim",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
